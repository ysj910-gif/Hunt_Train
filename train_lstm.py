import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import tkinter as tk
from tkinter import filedialog
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os

# === [1] ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (Dropout + Future Steps + ë²„ê·¸ ìˆ˜ì •) ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, future_steps=1, dropout=0.3):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.future_steps = future_steps
        self.num_classes = num_classes  # [ìˆ˜ì •] ëˆ„ë½ë˜ì—ˆë˜ ë³€ìˆ˜ ì¶”ê°€
        
        # LSTM ë ˆì´ì–´ (Deep Structure + Dropout)
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # ì¶œë ¥ì¸µ: ë¯¸ë˜ì˜ Nê°œ í–‰ë™ì„ ëª¨ë‘ ì˜ˆì¸¡ (Many-to-Many í˜•íƒœ)
        self.fc = nn.Linear(hidden_size, num_classes * future_steps)

    def forward(self, x):
        # ì´ˆê¸°í™”
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTM ì‹¤í–‰
        out, _ = self.lstm(x, (h0, c0))
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ íˆë“  ìŠ¤í…Œì´íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ì „ì²´ë¥¼ ì˜ˆì¸¡
        out = self.fc(out[:, -1, :])
        
        # (Batch, Future_Steps, Num_Classes) í˜•íƒœë¡œ ë³€í™˜
        return out.reshape(-1, self.future_steps, self.num_classes)

# === [2] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ===
SEQ_LENGTH = 200      # ê³¼ê±° 200í”„ë ˆì„(ì•½ 6ì´ˆ)ì„ ë³´ê³  íŒë‹¨ (Long Term Memory)
FUTURE_STEPS = 30     # ë¯¸ë˜ 30í”„ë ˆì„(ì•½ 1ì´ˆ)ì˜ í–‰ë™ì„ ë¯¸ë¦¬ ê³„íš
HIDDEN_SIZE = 256     # ë‡Œ ìš©ëŸ‰ (High Capacity)
NUM_LAYERS = 4        # 4ì¸µ êµ¬ì¡° (Very Deep Learning)
DROPOUT = 0.3         # ê³¼ì í•© ë°©ì§€ (30% ë§ê°)
EPOCHS = 500          # í•™ìŠµ íšŸìˆ˜
BATCH_SIZE = 64       # ë°°ì¹˜ í¬ê¸°
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# [í•µì‹¬] í•™ìŠµì— ì‚¬ìš©í•  íŠ¹ì„± (upgrade_data.py ê²°ê³¼ë¬¼)
FEATURE_COLS = [
    'player_x', 'player_y', 'entropy', 'platform_id', 'ult_ready', 'sub_ready',
    # ìœ„ê¸° ê°ì§€ ì„¼ì„œ (ê±°ë¦¬ ì—­ìˆ˜)
    'inv_dist_up', 'inv_dist_down', 'inv_dist_left', 'inv_dist_right',
    # ë„¤ë¹„ê²Œì´ì…˜ ì„¼ì„œ (ëª¨ì„œë¦¬ ê±°ë¦¬)
    'corner_tl', 'corner_tr', 'corner_bl', 'corner_br'
]
TARGET_COL = 'key_pressed'

# === [3] ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜ ===
def create_sequences(df, seq_length, future_steps, scaler, encoder):
    # íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€ (í˜¸í™˜ì„± ìœ ì§€)
    for col in FEATURE_COLS:
        if col not in df.columns: df[col] = 0
            
    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (DataFrame í˜•íƒœ ìœ ì§€í•˜ì—¬ ê²½ê³  ë°©ì§€)
    data_scaled = scaler.transform(df[FEATURE_COLS])
    
    # íƒ€ê²Ÿ ì¸ì½”ë”©
    target_values = encoder.transform(df[TARGET_COL].astype(str).values)
    
    xs, ys = [], []
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
    if len(df) <= seq_length + future_steps:
        return np.array([]), np.array([])

    # Sliding Window ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ìƒì„±
    for i in range(len(df) - seq_length - future_steps + 1):
        x_window = data_scaled[i : i + seq_length]
        y_window = target_values[i + seq_length : i + seq_length + future_steps]
        xs.append(x_window)
        ys.append(y_window)
        
    return np.array(xs), np.array(ys)

# === [4] ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ===
def train():
    # 1. íŒŒì¼ ì„ íƒ
    root = tk.Tk(); root.withdraw()
    print("ğŸ“‚ í•™ìŠµí•  CSV ë°ì´í„° íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (upgrade_data.pyë¡œ ë³€í™˜ëœ íŒŒì¼ ê¶Œì¥)...")
    files = filedialog.askopenfilenames(title="CSV ì„ íƒ", filetypes=[("CSV", "*.csv")])
    if not files: return

    # 2. ë°ì´í„° ë¡œë“œ ë° ë³‘í•©
    print("â³ ë°ì´í„° ë¡œë“œ ì¤‘...")
    temp_dfs = []
    for f in files:
        try:
            df = pd.read_csv(f)
            # ë…¸ì´ì¦ˆ ë°ì´í„° ì œê±°
            ignore_keys = ['media_volume_up', 'esc', 'f1', 'alt_l', 'caps_lock', 'unknown']
            df = df[~df['key_pressed'].isin(ignore_keys)]
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df['key_pressed'] = df['key_pressed'].fillna('None')
            df['platform_id'] = df['platform_id'].fillna(-1)
            
            temp_dfs.append(df)
        except Exception as e:
            print(f"âš ï¸ ë¡œë“œ ì‹¤íŒ¨ ({os.path.basename(f)}): {e}")
            
    if not temp_dfs: 
        print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return
        
    full_df = pd.concat(temp_dfs, ignore_index=True)
    
    # 3. Scaler & Encoder í•™ìŠµ
    print("âš–ï¸ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (íŠ¹ì´ê°’ ë³´ì •) ì¤‘...")
    # ì „ì²´ ë°ì´í„°ì…‹ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
    for col in FEATURE_COLS:
        if col not in full_df.columns: full_df[col] = 0
            
    scaler = StandardScaler()
    scaler.fit(full_df[FEATURE_COLS])
    
    encoder = LabelEncoder()
    encoder.fit(full_df[TARGET_COL].astype(str))
    
    num_classes = len(encoder.classes_)
    print(f"ğŸ·ï¸ í´ë˜ìŠ¤: {num_classes}ê°œ, íŠ¹ì„±: {len(FEATURE_COLS)}ê°œ (ê³ ê¸‰ ê±°ë¦¬ ì„¼ì„œ í¬í•¨)")

    # 4. ì‹œí€€ìŠ¤ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ ê³ ë ¤)
    print(f"âœ‚ï¸ ì‹œí€€ìŠ¤ ë³€í™˜ ì¤‘ (Seq: {SEQ_LENGTH}, Future: {FUTURE_STEPS})...")
    X_list, y_list = [], []
    
    for df in temp_dfs:
        xs, ys = create_sequences(df, SEQ_LENGTH, FUTURE_STEPS, scaler, encoder)
        if len(xs) > 0:
            X_list.append(xs)
            y_list.append(ys)
            
    if not X_list:
        print("âŒ í•™ìŠµ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    X_all = np.concatenate(X_list, axis=0)
    y_all = np.concatenate(y_list, axis=0)
    
    # 5. ë°ì´í„°ì…‹ ì¤€ë¹„ (Train/Test Split)
    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, shuffle=True, random_state=42)

    train_dataset = TensorDataset(torch.FloatTensor(X_train).to(DEVICE), torch.LongTensor(y_train).to(DEVICE))
    test_dataset = TensorDataset(torch.FloatTensor(X_test).to(DEVICE), torch.LongTensor(y_test).to(DEVICE))
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 6. ëª¨ë¸ ìƒì„± ë° ì„¤ì •
    model = LSTMModel(
        input_size=len(FEATURE_COLS),
        hidden_size=HIDDEN_SIZE,
        num_layers=NUM_LAYERS,
        num_classes=num_classes,
        future_steps=FUTURE_STEPS,
        dropout=DROPOUT
    ).to(DEVICE)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # [ì•ˆì •í™”] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì„±ëŠ¥ ì •ì²´ ì‹œ í•™ìŠµë¥  ê°ì†Œ)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)

    # 7. í•™ìŠµ ë£¨í”„
    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘ (Device: {DEVICE})")
    
    best_acc = 0.0
    best_model_state = None
    
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        
        for bx, by in train_loader:
            optimizer.zero_grad()
            outputs = model(bx) # (Batch, Future, Classes)
            
            # Loss ê³„ì‚°: (Batch * Future, Classes) í˜•íƒœë¡œ í¼ì³ì„œ ê³„ì‚°
            loss = criterion(outputs.view(-1, num_classes), by.view(-1))
            
            loss.backward()
            
            # [ì•ˆì •í™”] Gradient Clipping (Loss í­ë°œ ë°©ì§€)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += loss.item()
            
        # ê²€ì¦ (Validation)
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for bx, by in test_loader:
                outputs = model(bx) # (Batch, Future, Classes)
                _, predicted = torch.max(outputs, 2) # (Batch, Future)
                
                # ëª¨ë“  íƒ€ì„ìŠ¤í…ì˜ ì˜ˆì¸¡ì´ ë§ëŠ”ì§€ í™•ì¸ (ì „ì²´ ì •í™•ë„)
                correct += (predicted == by).sum().item()
                total += by.numel() # Batch * Future
        
        acc = 100 * correct / total
        avg_loss = train_loss / len(train_loader)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(acc)

        # ìµœê³  ê¸°ë¡ ì €ì¥
        if acc > best_acc:
            best_acc = acc
            best_model_state = model.state_dict()
            print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f} | Val Acc: {acc:.2f}% (â­ New Best!)")
        else:
            if (epoch+1) % 5 == 0:
                print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f} | Val Acc: {acc:.2f}%")

    # 8. ì €ì¥ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
    print(f"\nğŸ’¾ ìµœê³  ì •í™•ë„({best_acc:.2f}%) ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    save_path = "kinesis_lstm_best.pth"
    
    # ì €ì¥í•  ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨
    save_dict = {
        'model_state': best_model_state,
        'scaler': scaler,
        'encoder': encoder,
        'feature_cols': FEATURE_COLS,
        'input_size': len(FEATURE_COLS),
        'hidden_size': HIDDEN_SIZE,
        'num_layers': NUM_LAYERS,
        'num_classes': num_classes,
        'seq_length': SEQ_LENGTH,
        'future_steps': FUTURE_STEPS,
        'dropout': DROPOUT
    }
    
    torch.save(save_dict, save_path)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    train()