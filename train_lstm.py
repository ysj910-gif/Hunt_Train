# train_lstm.py
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import tkinter as tk
from tkinter import filedialog
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os

# === ì„¤ì • ===
SEQ_LENGTH = 200  # ê³¼ê±° 10í”„ë ˆì„ì„ ë³´ê³  íŒë‹¨ (ì•½ 0.3~0.5ì´ˆ)
HIDDEN_SIZE = 256
NUM_LAYERS = 3
EPOCHS = 100      # í•™ìŠµ íšŸìˆ˜
BATCH_SIZE = 64
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"ğŸš€ í•™ìŠµ ì¥ì¹˜: {DEVICE} (4070 Superë¼ë©´ 'cuda'ê°€ ë– ì•¼ í•©ë‹ˆë‹¤)")

class MapleDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        # x shape: (batch, seq_len, input_size)
        h0 = torch.zeros(NUM_LAYERS, x.size(0), HIDDEN_SIZE).to(DEVICE)
        c0 = torch.zeros(NUM_LAYERS, x.size(0), HIDDEN_SIZE).to(DEVICE)
        
        out, _ = self.lstm(x, (h0, c0))
        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ì˜ ê²°ê³¼ë§Œ ì‚¬ìš©
        out = self.fc(out[:, -1, :])
        return out

def create_sequences(data, target, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = target[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

def train():
    # 1. íŒŒì¼ ì„ íƒ
    root = tk.Tk(); root.withdraw()
    print("ğŸ“‚ í•™ìŠµí•  CSV ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)...")
    files = filedialog.askopenfilenames(title="CSV ì„ íƒ", filetypes=[("CSV", "*.csv")])
    if not files: return

    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df_list = []
    for f in files:
        try:
            df_list.append(pd.read_csv(f))
        except Exception as e:
            print(f"âš ï¸ ë¡œë“œ ì‹¤íŒ¨ ({f}): {e}")
            
    if not df_list: return
    df = pd.concat(df_list, ignore_index=True)
    print(f"ğŸ“Š ì´ ë°ì´í„°: {len(df)}ê°œ")

    # ë…¸ì´ì¦ˆ ì œê±° (ë¶ˆí•„ìš”í•œ í‚¤)
    ignore_keys = ['media_volume_up', 'esc', 'f1', 'alt_l', 'caps_lock']
    df = df[~df['key_pressed'].isin(ignore_keys)]

    df = df[df['key_pressed'] != 'down']

    none_df = df[df['key_pressed'] == 'None'].sample(frac=0.1, random_state=42)
    action_df = df[df['key_pressed'] != 'None']
    df = pd.concat([none_df, action_df])
    
    feature_cols = ['player_x', 'player_y', 'entropy', 'platform_id', 'ult_ready', 'sub_ready']
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° í˜•ë³€í™˜
    for col in feature_cols:
        if col not in df.columns: df[col] = 0
    
    df[feature_cols] = df[feature_cols].fillna(0)
    df['key_pressed'] = df['key_pressed'].fillna('None').astype(str)

    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[feature_cols])
    
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(df['key_pressed'])

    # 3. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (ìœˆë„ìš° ìŠ¬ë¼ì´ë”©)
    print("â³ ì‹œí€€ìŠ¤ ë°ì´í„° ë³€í™˜ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
    X_seq, y_seq = create_sequences(X_scaled, y_encoded, SEQ_LENGTH)
    
    # [í•µì‹¬ ìˆ˜ì •] ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ì€(2ê°œ ë¯¸ë§Œ) í´ë˜ìŠ¤ ì œê±°
    unique, counts = np.unique(y_seq, return_counts=True)
    rare_classes = unique[counts < 2]
    
    if len(rare_classes) > 0:
        print(f"âš ï¸ [ìë™ ë³´ì •] ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•œ í¬ê·€ í–‰ë™ {len(rare_classes)}ì¢…ë¥˜ë¥¼ í•™ìŠµì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
        # í¬ê·€ í´ë˜ìŠ¤ê°€ ì•„ë‹Œ ë°ì´í„°ë§Œ ë‚¨ê¹€
        mask = np.isin(y_seq, rare_classes, invert=True)
        X_seq = X_seq[mask]
        y_seq = y_seq[mask]

    # 4. í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=True, stratify=y_seq)

    train_dataset = MapleDataset(X_train, y_train)
    test_dataset = MapleDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 5. ëª¨ë¸ ì´ˆê¸°í™”
    num_classes = len(encoder.classes_)
    model = LSTMModel(len(feature_cols), HIDDEN_SIZE, NUM_LAYERS, num_classes).to(DEVICE)
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì˜µì…˜)
    # class_counts = torch.bincount(torch.tensor(y_train))
    # weights = 1. / (class_counts.float() + 1e-6)
    # criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE))
    criterion = nn.CrossEntropyLoss()
    
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 6. í•™ìŠµ ë£¨í”„
    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘ (Total Epochs: {EPOCHS})")
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # ê²€ì¦
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)
                outputs = model(X_batch)
                _, predicted = torch.max(outputs.data, 1)
                total += y_batch.size(0)
                correct += (predicted == y_batch).sum().item()
        
        acc = 100 * correct / total
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {train_loss/len(train_loader):.4f} | Accuracy: {acc:.2f}%")

    # 7. ì €ì¥
    save_path = "kinesis_lstm_model.pth"
    save_dict = {
        'model_state': model.state_dict(),
        'scaler': scaler,
        'encoder': encoder,
        'seq_length': SEQ_LENGTH,
        'input_size': len(feature_cols),
        'hidden_size': HIDDEN_SIZE,
        'num_layers': NUM_LAYERS,
        'num_classes': num_classes,
        'feature_cols': feature_cols
    }
    
    torch.save(save_dict, save_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    train()