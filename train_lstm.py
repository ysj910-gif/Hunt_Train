import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import tkinter as tk
from tkinter import filedialog
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
import os
import json

# === [1] ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, future_steps=1, dropout=0.3):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.future_steps = future_steps
        self.num_classes = num_classes
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, num_classes * future_steps)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out.reshape(-1, self.future_steps, self.num_classes)

# === [2] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ===
SEQ_LENGTH = 150       # ê³¼ê±° 150í”„ë ˆì„(ì•½ 4~5ì´ˆ)ì„ ë³´ê³  íŒë‹¨
FUTURE_STEPS = 30      # ë¯¸ë˜ 30í”„ë ˆì„(ì•½ 1ì´ˆ) ì˜ˆì¸¡
HIDDEN_SIZE = 256      # ëª¨ë¸ ìš©ëŸ‰
NUM_LAYERS = 4         # ë ˆì´ì–´ ê¹Šì´
DROPOUT = 0.3
EPOCHS = 300
BATCH_SIZE = 64
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
GAMMA = 0.98           # ë³´ìƒ ê°ê°€ìœ¨ (ë¯¸ë˜ì˜ í‚¬ì„ í˜„ì¬ ê°€ì¹˜ë¡œ í™˜ì‚°í•  ë•Œ ì‚¬ìš©)

# í•™ìŠµì— ì‚¬ìš©í•  íŠ¹ì„± ì»¬ëŸ¼ (upgrade_data.py ê²°ê³¼ë¬¼)
FEATURE_COLS = [
    'player_x', 'player_y', 'entropy', 'platform_id', 'ult_ready', 'sub_ready',
    'inv_dist_up', 'inv_dist_down', 'inv_dist_left', 'inv_dist_right',
    'corner_tl', 'corner_tr', 'corner_bl', 'corner_br'
]
TARGET_COL = 'key_pressed'

# === [3] ì„¤ì • íŒŒì¼ ë¡œë“œ (ì„¤ì¹˜ê¸° ì¸ì‹ìš©) ===
def load_install_skills():
    """hunter_config.jsonì—ì„œ ì§€ì†ì‹œê°„(dur)ì´ 2ì´ˆ ì´ìƒì¸ ìŠ¤í‚¬ì„ ì„¤ì¹˜ê¸°ë¡œ ì¸ì‹"""
    config_path = "hunter_config.json"
    install_skills = {} # { 'key': duration }
    
    if not os.path.exists(config_path):
        print("âš ï¸ ì„¤ì • íŒŒì¼(hunter_config.json)ì´ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ê¸° í•™ìŠµ ê¸°ëŠ¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {}
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            mapping = data.get("mapping", {})
            
            print("\nğŸ”§ [ìŠ¤í‚¬ ì„¤ì • ë¡œë“œ]")
            for name, info in mapping.items():
                key = info.get("key", "").lower()
                dur = float(info.get("dur", 0))
                
                # ì§€ì†ì‹œê°„ì´ 2.0ì´ˆ ì´ìƒì´ë©´ ì„¤ì¹˜ê¸°ë¡œ ê°„ì£¼
                if key and dur >= 2.0:
                    install_skills[key] = dur
                    print(f"   - ì„¤ì¹˜ê¸° ê°ì§€: [{key.upper()}] {name} (ì§€ì† {dur}ì´ˆ)")
                    
        return install_skills
    except Exception as e:
        print(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

# === [4] ìŠ¤ë§ˆíŠ¸ ë³´ìƒ ê³„ì‚° (Elite Data Filtering í•µì‹¬) ===
def calculate_smart_rewards(df, install_skills, gamma=0.98):
    """
    1. ì¼ë°˜ ê³µê²©: í‚¬ì´ ë°œìƒí•˜ë©´ ê·¸ ì§ì „ í–‰ë™ë“¤ì— ì ìˆ˜ ë¶€ì—¬ (Backpropagation)
    2. ì„¤ì¹˜ê¸°: ì„¤ì¹˜ í›„ ì§€ì†ì‹œê°„ ë™ì•ˆ ë°œìƒí•œ ì´ í‚¬ ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ì ìˆ˜ ë¶€ì—¬
    """
    timestamps = df['timestamp'].values
    actions = df['key_pressed'].fillna('None').astype(str).values
    
    # kill_reward ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
    if 'kill_reward' not in df.columns:
        if 'kill_count' in df.columns:
            # kill_countì˜ ë³€í™”ëŸ‰ìœ¼ë¡œ reward ê³„ì‚° (ìŒìˆ˜ ì œê±°)
            rewards = df['kill_count'].diff().fillna(0).values
            rewards[rewards < 0] = 0
        else:
            rewards = np.zeros(len(df))
    else:
        rewards = df['kill_reward'].values

    # [1] ê¸°ë³¸ ë‹¨ê¸° ë³´ìƒ (ì¼ë°˜ ê³µê²©ìš©)
    discounted = np.zeros_like(rewards, dtype=np.float32)
    running_add = 0
    # ë’¤ì—ì„œë¶€í„° ì•ìœ¼ë¡œ ê³„ì‚° (ë‚˜ì¤‘ì— ì¡ì€ í‚¬ ì ìˆ˜ë¥¼ ì•ìª½ í–‰ë™ì— ë‚˜ëˆ ì¤Œ)
    for t in reversed(range(len(rewards))):
        if rewards[t] > 0:
            running_add = rewards[t]
        else:
            running_add = running_add * gamma
        discounted[t] = running_add

    # [2] ì„¤ì¹˜ê¸° ì¥ê¸° ë³´ìƒ (ì„¤ì • íŒŒì¼ ê¸°ë°˜)
    if install_skills:
        for t in range(len(df)):
            action = actions[t].lower()
            
            # í˜„ì¬ ëˆ„ë¥¸ í‚¤ê°€ ì„¤ì¹˜ê¸°ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸ (ì˜ˆ: 'down+e' -> 'e')
            matched_dur = 0
            for k, dur in install_skills.items():
                if k in action: 
                    matched_dur = dur
                    break
            
            if matched_dur > 0:
                current_time = timestamps[t]
                future_kills = 0
                
                # ì„¤ì¹˜ê¸° ì§€ì†ì‹œê°„ ë™ì•ˆ ë¯¸ë˜ì˜ í‚¬ì„ ë¯¸ë¦¬ ë‚´ë‹¤ë´„
                for future_t in range(t + 1, len(df)):
                    if timestamps[future_t] - current_time > matched_dur:
                        break
                    future_kills += rewards[future_t]
                
                # ë³´ìƒ ì •ì±…: ì„¤ì¹˜ê¸° í•˜ë‚˜ë¡œ 3ë§ˆë¦¬ ì´ìƒ ì¡ì•„ì•¼ ì´ë“
                if future_kills >= 3: 
                    bonus = future_kills * 3.0 # ê°•ë ¥í•œ ë³´ë„ˆìŠ¤
                    discounted[t] += bonus
                else:
                    discounted[t] -= 5.0 # ë‚­ë¹„ ì‹œ ê°•ë ¥í•œ íŒ¨ë„í‹° (ì“°ì§€ ë§ˆ!)
    
    df['discounted_reward'] = discounted
    return df

# === [5] ì‹œí€€ìŠ¤ ìƒì„± (ë°ì´í„° ì„ ë³„) ===
def create_sequences_smart(df, seq_length, future_steps, scaler, encoder):
    # íŠ¹ì„± ì»¬ëŸ¼ ì±„ìš°ê¸°
    for col in FEATURE_COLS:
        if col not in df.columns: df[col] = 0
            
    data_scaled = scaler.transform(df[FEATURE_COLS])
    target_values = encoder.transform(df[TARGET_COL].astype(str).values)
    values = df['discounted_reward'].values
    
    xs, ys = [], []
    for i in range(len(df) - seq_length - future_steps + 1):
        target_idx = i + seq_length
        
        # [í•µì‹¬ í•„í„°ë§]
        # í•´ë‹¹ ì‹œì ì˜ í–‰ë™ ê°€ì¹˜(Reward)ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´(0.01 ì´í•˜) -> ì“¸ëª¨ì—†ëŠ” í–‰ë™
        # ì“¸ëª¨ì—†ëŠ” í–‰ë™ì€ 90% í™•ë¥ ë¡œ í•™ìŠµ ë°ì´í„°ì—ì„œ ì œì™¸ (ê³¼ê°í•œ ì‚­ì œ)
        if values[target_idx] <= 0.01 and np.random.rand() > 0.1:
            continue
            
        x_window = data_scaled[i : i + seq_length]
        y_window = target_values[i + seq_length : i + seq_length + future_steps]
        xs.append(x_window)
        ys.append(y_window)
        
    return np.array(xs), np.array(ys)

# === [6] ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ===
def train():
    root = tk.Tk(); root.withdraw()
    
    # 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
    install_skills = load_install_skills()
    
    print("\nğŸ“‚ í•™ìŠµí•  CSV ë°ì´í„° íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (upgrade_data.py ë³€í™˜ íŒŒì¼ ê¶Œì¥)...")
    files = filedialog.askopenfilenames(title="CSV ì„ íƒ", filetypes=[("CSV", "*.csv")])
    if not files: return

    print("â³ ë°ì´í„° ë¡œë“œ ë° ìŠ¤ë§ˆíŠ¸ ë³´ìƒ ê³„ì‚° ì¤‘...")
    temp_dfs = []
    
    for f in files:
        try:
            df = pd.read_csv(f)
            # ë…¸ì´ì¦ˆ í‚¤ ì œê±°
            ignore_keys = ['media_volume_up', 'esc', 'f1', 'alt_l', 'caps_lock', 'unknown']
            df = df[~df['key_pressed'].isin(ignore_keys)]
            df['key_pressed'] = df['key_pressed'].fillna('None')
            df['platform_id'] = df['platform_id'].fillna(-1)
            
            # [ë³´ìƒ ê³„ì‚°] ì—¬ê¸°ì„œ 'ì˜í•œ í–‰ë™'ì— ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
            df = calculate_smart_rewards(df, install_skills, gamma=GAMMA)
            
            # (ë””ë²„ê·¸) ìµœëŒ€ ë³´ìƒ ì ìˆ˜ ì¶œë ¥
            max_r = df['discounted_reward'].max()
            print(f"   - {os.path.basename(f)}: ìµœëŒ€ ê°€ì¹˜ ì ìˆ˜ {max_r:.2f}")
            
            temp_dfs.append(df)
        except Exception as e:
            print(f"âš ï¸ ë¡œë“œ ì‹¤íŒ¨ ({os.path.basename(f)}): {e}")
            
    if not temp_dfs: return
    full_df = pd.concat(temp_dfs, ignore_index=True)
    
    # 2. ìŠ¤ì¼€ì¼ëŸ¬ & ì¸ì½”ë” í•™ìŠµ
    scaler = StandardScaler()
    scaler.fit(full_df[FEATURE_COLS])
    encoder = LabelEncoder()
    encoder.fit(full_df[TARGET_COL].astype(str))
    num_classes = len(encoder.classes_)

    # 3. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (í•„í„°ë§ ì ìš©)
    print(f"âœ‚ï¸ ì˜ë¯¸ ì—†ëŠ” êµ¬ê°„(Idle) ì œê±° ë° í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
    X_list, y_list = [], []
    for df in temp_dfs:
        xs, ys = create_sequences_smart(df, SEQ_LENGTH, FUTURE_STEPS, scaler, encoder)
        if len(xs) > 0:
            X_list.append(xs)
            y_list.append(ys)
            
    if not X_list: print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (ëª¨ë“  ë°ì´í„°ê°€ í•„í„°ë§ë¨)"); return

    X_all = np.concatenate(X_list, axis=0)
    y_all = np.concatenate(y_list, axis=0)
    print(f"âœ¨ ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(X_all)}ê°œ ì‹œí€€ìŠ¤ (ì‚¬ëƒ¥ íš¨ìœ¨ ìµœì í™”ë¨)")

    # 4. í•™ìŠµ ë°ì´í„°ì…‹ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, shuffle=True)
    train_dataset = TensorDataset(torch.FloatTensor(X_train).to(DEVICE), torch.LongTensor(y_train).to(DEVICE))
    test_dataset = TensorDataset(torch.FloatTensor(X_test).to(DEVICE), torch.LongTensor(y_test).to(DEVICE))
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 5. ëª¨ë¸ ìƒì„±
    model = LSTMModel(len(FEATURE_COLS), HIDDEN_SIZE, NUM_LAYERS, num_classes, FUTURE_STEPS, DROPOUT).to(DEVICE)
    
    # [ê°€ì¤‘ì¹˜ ì ìš©] 'None' í´ë˜ìŠ¤ëŠ” ì ìˆ˜ë¥¼ ê¹ì•„ì„œ ë” ì ê·¹ì ìœ¼ë¡œ ì›€ì§ì´ê²Œ ìœ ë„
    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_all.flatten()), y=y_all.flatten())
    try:
        none_idx = encoder.transform(['None'])[0]
        class_weights[none_idx] *= 0.1 # None ê°€ì¤‘ì¹˜ 1/10 í† ë§‰
        print(f"ğŸ”¥ 'None' í´ë˜ìŠ¤ íŒ¨ë„í‹° ì ìš©ë¨ (ì ê·¹ì„± ê°•í™”)")
    except: pass
    weights_tensor = torch.FloatTensor(class_weights).to(DEVICE)
    
    criterion = nn.CrossEntropyLoss(weight=weights_tensor)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

    # 6. í•™ìŠµ ë£¨í”„
    print(f"\nğŸ”¥ ì—˜ë¦¬íŠ¸ í•™ìŠµ ì‹œì‘...")
    best_acc = 0.0
    best_model_state = None
    
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            optimizer.zero_grad()
            outputs = model(bx)
            loss = criterion(outputs.view(-1, num_classes), by.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()
            
        model.eval()
        correct = 0; total = 0
        with torch.no_grad():
            for bx, by in test_loader:
                outputs = model(bx)
                _, predicted = torch.max(outputs, 2)
                correct += (predicted == by).sum().item()
                total += by.numel()
        
        acc = 100 * correct / total
        avg_loss = train_loss / len(train_loader)
        scheduler.step(acc)

        if acc > best_acc:
            best_acc = acc
            best_model_state = model.state_dict()
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Acc: {acc:.2f}% (â­ Best)")
        elif (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%")

    # 7. ëª¨ë¸ ì €ì¥
    save_path = "kinesis_lstm_best.pth"
    torch.save({
        'model_state': best_model_state,
        'scaler': scaler, 'encoder': encoder,
        'feature_cols': FEATURE_COLS,
        'input_size': len(FEATURE_COLS), 'hidden_size': HIDDEN_SIZE,
        'num_layers': NUM_LAYERS, 'num_classes': num_classes,
        'seq_length': SEQ_LENGTH, 'future_steps': FUTURE_STEPS, 'dropout': DROPOUT
    }, save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    train()